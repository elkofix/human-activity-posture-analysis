import streamlit as st
import cv2
import numpy as np
import pandas as pd
import mediapipe as mp
import joblib
import tempfile
import os
import time
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns

# Configure Streamlit page
st.set_page_config(
    page_title="Human Activity & Posture Analysis",
    page_icon="ü§∏‚Äç‚ôÇÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize MediaPipe Pose
@st.cache_resource
def init_mediapipe():
    mp_pose = mp.solutions.pose
    mp_drawing = mp.solutions.drawing_utils
    pose = mp_pose.Pose(
        static_image_mode=False,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5
    )
    return mp_pose, mp_drawing, pose

# Load the trained model
@st.cache_resource
def load_model():
    model_path = os.path.join(os.path.dirname(__file__), 'models', 'pose_classification_model.joblib')
    try:
        model = joblib.load(model_path)
        return model
    except Exception as e:
        st.error(f"Error loading model: {e}")
        return None

# Angle calculation function (from notebook)
def calculate_angle(a, b, c):
    """Calculate angle between three points"""
    a = np.array(a)
    b = np.array(b)
    c = np.array(c)
    
    ba = a - b
    bc = c - b
    
    dot_product = np.dot(ba, bc)
    norm_product = np.linalg.norm(ba) * np.linalg.norm(bc)
    
    epsilon = 1e-7
    cosine_angle = dot_product / (norm_product + epsilon)
    cosine_angle = np.clip(cosine_angle, -1.0, 1.0)
    
    angle = np.arccos(cosine_angle)
    return np.degrees(angle)

# Landmark extraction (from notebook)
def extract_features_from_landmarks(landmarks):
    """Extract angle features from pose landmarks"""
    if landmarks is None:
        return None
    
    # Define landmark indices (same as notebook)
    LANDMARKS = {
        'LEFT_SHOULDER': 11, 'RIGHT_SHOULDER': 12,
        'LEFT_ELBOW': 13,    'RIGHT_ELBOW': 14,
        'LEFT_WRIST': 15,    'RIGHT_WRIST': 16,
        'LEFT_HIP': 23,      'RIGHT_HIP': 24,
        'LEFT_KNEE': 25,     'RIGHT_KNEE': 26,
        'LEFT_ANKLE': 27,    'RIGHT_ANKLE': 28,
    }
    
    # Extract coordinates
    coords = {}
    for name, idx in LANDMARKS.items():
        landmark = landmarks.landmark[idx]
        coords[name] = [landmark.x, landmark.y, landmark.z]
    
    # Calculate angles
    angles = {
        'angle_left_elbow': calculate_angle(coords['LEFT_SHOULDER'], coords['LEFT_ELBOW'], coords['LEFT_WRIST']),
        'angle_right_elbow': calculate_angle(coords['RIGHT_SHOULDER'], coords['RIGHT_ELBOW'], coords['RIGHT_WRIST']),
        'angle_left_shoulder': calculate_angle(coords['LEFT_ELBOW'], coords['LEFT_SHOULDER'], coords['LEFT_HIP']),
        'angle_right_shoulder': calculate_angle(coords['RIGHT_ELBOW'], coords['RIGHT_SHOULDER'], coords['RIGHT_HIP']),
        'angle_left_hip': calculate_angle(coords['LEFT_SHOULDER'], coords['LEFT_HIP'], coords['LEFT_KNEE']),
        'angle_right_hip': calculate_angle(coords['RIGHT_SHOULDER'], coords['RIGHT_HIP'], coords['RIGHT_KNEE']),
        'angle_left_knee': calculate_angle(coords['LEFT_HIP'], coords['LEFT_KNEE'], coords['LEFT_ANKLE']),
        'angle_right_knee': calculate_angle(coords['RIGHT_HIP'], coords['RIGHT_KNEE'], coords['RIGHT_ANKLE']),
    }
    
    return angles

def process_video(video_file, mp_pose, mp_drawing, pose, model):
    """Process video file and return predictions"""
    # Save uploaded file temporarily
    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
        tmp_file.write(video_file.read())
        tmp_path = tmp_file.name
    
    try:
        cap = cv2.VideoCapture(tmp_path)
        predictions = []
        frame_count = 0
        processed_frames = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        while cap.isOpened() and frame_count < min(total_frames, 100):  # Limit to 100 frames for demo
            ret, frame = cap.read()
            if not ret:
                break
            
            # Process frame
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose.process(frame_rgb)
            
            if results.pose_landmarks:
                # Extract features
                features = extract_features_from_landmarks(results.pose_landmarks)
                if features:
                    # Make prediction
                    features_df = pd.DataFrame([features])
                    prediction = model.predict(features_df)[0]
                    predictions.append(prediction)
                    
                    # Draw landmarks on frame
                    mp_drawing.draw_landmarks(
                        frame,
                        results.pose_landmarks,
                        mp_pose.POSE_CONNECTIONS,
                        landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),
                        connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2)
                    )
                    
                    # Add prediction text
                    cv2.putText(frame, f"Prediction: {prediction}", (10, 30), 
                              cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                    
                    processed_frames.append(frame)
            
            frame_count += 1
            progress_bar.progress(frame_count / min(total_frames, 100))
            status_text.text(f"Processing frame {frame_count}/{min(total_frames, 100)}")
        
        cap.release()
        os.unlink(tmp_path)  # Clean up temporary file
        
        return predictions, processed_frames
        
    except Exception as e:
        st.error(f"Error processing video: {e}")
        return [], []

def process_image(image_file, mp_pose, mp_drawing, pose, model):
    """Process single image and return prediction"""
    try:
        # Convert PIL image to numpy array
        image = Image.open(image_file)
        image_np = np.array(image)
        
        # Convert to RGB if needed
        if len(image_np.shape) == 3 and image_np.shape[2] == 3:
            image_rgb = image_np
        else:
            image_rgb = cv2.cvtColor(image_np, cv2.COLOR_BGR2RGB)
        
        # Process with MediaPipe
        results = pose.process(image_rgb)
        
        if results.pose_landmarks:
            # Extract features
            features = extract_features_from_landmarks(results.pose_landmarks)
            if features:
                # Make prediction
                features_df = pd.DataFrame([features])
                prediction = model.predict(features_df)[0]
                prediction_proba = model.predict_proba(features_df)[0]
                
                # Draw landmarks
                image_with_landmarks = image_np.copy()
                mp_drawing.draw_landmarks(
                    image_with_landmarks,
                    results.pose_landmarks,
                    mp_pose.POSE_CONNECTIONS,
                    landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),
                    connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2)
                )
                
                return prediction, prediction_proba, features, image_with_landmarks
        
        return None, None, None, None
        
    except Exception as e:
        st.error(f"Error processing image: {e}")
        return None, None, None, None

def main():
    st.title("üé• Live Human Activity Detection")
    st.markdown("### Real-time AI-powered pose classification using your camera")
    
    # Load model and MediaPipe
    model = load_model()
    if model is None:
        st.error("Could not load the trained model. Please check the model file.")
        return
    
    mp_pose, mp_drawing, pose = init_mediapipe()
    
    # Sidebar
    st.sidebar.header("ü§∏‚Äç‚ôÇÔ∏è About the App")
    st.sidebar.markdown("""
    **üéØ Main Feature: Live Camera Detection**
    
    Turn on your camera for real-time activity recognition!
    
    This app uses a **Random Forest Classifier** trained on pose landmarks extracted using **MediaPipe**.
    
    **Recognized Activities:**
    - üö∂ Caminar hacia la c√°mara (Walk towards camera)
    - üîô Caminar de regreso (Walk away)
    - üîÑ Girar (Turn around)
    - üí∫ Sentarse (Sit down)
    - üèÉ Ponerse de pie (Stand up)
    
    **Features:** 8 joint angles (elbows, shoulders, hips, knees)
    
    **Model Performance:** 98% accuracy on test data
    """)
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("### üé• Camera Requirements")
    st.sidebar.markdown("""
    - Webcam access permission
    - Good lighting conditions  
    - Clear view of full body
    - Minimal background movement
    """);
    
    # Main interface
    tab1, tab2, tab3, tab4 = st.tabs(["üìπ Live Camera", "üì∏ Image Analysis", "üé• Video Analysis", "üìä Model Information"])
    
    with tab1:
        st.header("üé• Live Camera Detection")
        
        # Camera controls in a more prominent layout
        control_col1, control_col2, control_col3 = st.columns([1, 1, 1])
        
        with control_col1:
            start_camera = st.button("üî¥ Start Camera", type="primary", use_container_width=True, help="Click to start real-time detection")
        
        with control_col2:
            stop_camera = st.button("‚èπÔ∏è Stop Camera", use_container_width=True, help="Click to stop camera")
        
        with control_col3:
            if st.session_state.get('camera_active', False):
                st.success("üü¢ Camera Active")
            else:
                st.info("‚ö´ Camera Inactive")
                
        st.markdown("---")
        
        # Initialize session state for camera
        if 'camera_active' not in st.session_state:
            st.session_state.camera_active = False
        if 'camera_thread' not in st.session_state:
            st.session_state.camera_thread = None
        
        # Handle camera controls
        if start_camera:
            st.session_state.camera_active = True
            st.rerun()
        
        if stop_camera:
            st.session_state.camera_active = False
            if st.session_state.camera_thread:
                st.session_state.camera_thread = None
            st.rerun()
        
        if st.session_state.camera_active:
            st.markdown("### üî¥ **LIVE** - Real-time Activity Detection")
            
            # Show loading state
            with st.spinner("üîß Initializing camera..."):
                time.sleep(0.5)  # Brief pause for visual feedback
            
            # Create placeholders for live updates
            status_placeholder = st.empty()
            
            # Show camera initialization status
            status_placeholder.info("üì∑ Connecting to camera...")
            
            # Live camera detection
            try:
                cap = cv2.VideoCapture(0)  # Use default camera
                
                if not cap.isOpened():
                    status_placeholder.error("‚ùå Could not access camera. Please check your camera permissions and try again.")
                    st.session_state.camera_active = False
                else:
                    status_placeholder.success("‚úÖ Camera connected! Optimizing settings...")
                    
                    # Camera settings optimized for maximum fluid performance
                    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  # Higher resolution for better quality
                    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                    cap.set(cv2.CAP_PROP_FPS, 30)  # Higher FPS for maximum smoothness
                    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Reduce buffer to prevent lag
                    cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))  # Better codec
                    cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0.25)  # Faster exposure for consistent frame rate
                    
                    frame_count = 0
                    predictions_history = []
                    processing_frame = False
                    
                    # Clear status and show ready state
                    time.sleep(0.5)
                    status_placeholder.success("üé• Camera ready! Stand in front of camera and start moving...")
                    
                    # Create better layout for camera and info
                    main_col1, main_col2 = st.columns([2, 1])  # 2:1 ratio for camera:info
                    
                    with main_col1:
                        camera_container = st.container()
                        camera_display = camera_container.empty()
                    
                    with main_col2:
                        info_container = st.container()
                        with info_container:
                            st.markdown("### üìä Live Stats")
                            prediction_display = st.empty()
                            confidence_display = st.empty()
                            stats_display = st.empty()
                    
                    # Real-time processing loop
                    while st.session_state.camera_active:
                        ret, frame = cap.read()
                        if not ret:
                            status_placeholder.error("‚ùå Failed to read from camera")
                            break
                        
                        # Flip frame horizontally for mirror effect
                        frame = cv2.flip(frame, 1)
                        
                        # Show frame immediately for fluid display (clean video without landmarks)
                        frame_rgb_display = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        camera_display.image(frame_rgb_display, channels="RGB", use_container_width=True)
                        
                        # Process every 4th frame for AI analysis (but show all frames for smoothness)
                        if frame_count % 4 == 0 and not processing_frame:
                            processing_frame = True
                            
                            # Process pose detection in background (no visual overlay)
                            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                            results = pose.process(frame_rgb)
                            
                            current_prediction = "No pose detected"
                            confidence_scores = None
                            
                            if results.pose_landmarks:
                                # Extract features and predict
                                features = extract_features_from_landmarks(results.pose_landmarks)
                                if features:
                                    features_df = pd.DataFrame([features])
                                    prediction = model.predict(features_df)[0]
                                    prediction_proba = model.predict_proba(features_df)[0]
                                    
                                    current_prediction = prediction
                                    confidence_scores = prediction_proba
                                    predictions_history.append(prediction)
                                    
                                    # Keep only last 10 predictions for smoothing
                                    if len(predictions_history) > 10:
                                        predictions_history.pop(0)
                            
                            # Update status during processing (less frequently)
                            if frame_count % 20 == 0:  # Update status every 20 frames
                                if current_prediction != "No pose detected":
                                    status_placeholder.success(f"‚úÖ Detecting: {current_prediction}")
                                else:
                                    status_placeholder.warning("‚ö†Ô∏è No pose detected - move into camera view")
                            
                            # Update prediction info in sidebar
                            with prediction_display.container():
                                if current_prediction != "No pose detected":
                                    st.markdown(f"### üéØ {current_prediction}")
                                    
                                    # Show prediction history
                                    if len(predictions_history) > 3:
                                        recent_activities = list(set(predictions_history[-3:]))  # Last 3 unique activities
                                        st.markdown("**Recent:**")
                                        for activity in recent_activities[-3:]:
                                            st.markdown(f"‚Ä¢ {activity}")
                                else:
                                    st.markdown("### üîç Move into view")
                                    st.markdown("Position yourself in front of the camera")
                            
                            # Show frame stats
                            with stats_display.container():
                                st.markdown(f"**Frames:** {frame_count}")
                                st.markdown(f"**FPS:** ~{20 if frame_count > 0 else 0}")
                                if len(predictions_history) > 0:
                                    st.markdown(f"**Detections:** {len(predictions_history)}")
                            
                            processing_frame = False
                            
                            # Update confidence scores (less frequently for performance)
                            if confidence_scores is not None and frame_count % 10 == 0:  # Update every 10 frames
                                with confidence_display.container():
                                    st.markdown("**Confidence:**")
                                    
                                    # Create a simple progress bars for top predictions
                                    confidence_df = pd.DataFrame({
                                        'Activity': model.classes_,
                                        'Confidence': confidence_scores
                                    }).sort_values('Confidence', ascending=False)
                                    
                                    # Show top 3 predictions with compact progress bars
                                    for i, (_, row) in enumerate(confidence_df.head(3).iterrows()):
                                        activity_name = row['Activity']
                                        # Map to shorter display names
                                        if 'Caminando' in activity_name or 'Walking' in activity_name:
                                            activity_name = 'Walking'
                                        elif 'Girando' in activity_name or 'Turning' in activity_name:
                                            activity_name = 'Turning'
                                        elif 'Sentandome' in activity_name or 'Sitting' in activity_name:
                                            activity_name = 'Sitting'
                                        elif 'Levantarme' in activity_name or 'Standing' in activity_name:
                                            activity_name = 'Standing'
                                        else:
                                            activity_name = activity_name[:12] + '...' if len(activity_name) > 12 else activity_name
                                        
                                        st.progress(row['Confidence'], text=f"{activity_name}: {row['Confidence']:.0%}")
                        
                        frame_count += 1
                        
                        # Minimal status updates for maximum performance
                        if frame_count % 120 == 0:  # Show frame count every 120 frames (less frequent)
                            status_placeholder.info(f"üì∫ Running smoothly - {frame_count} frames")
                        
                        # No delay for maximum fluidity
                        # time.sleep removed for absolute maximum performance
                    
                    cap.release()
                    
            except Exception as e:
                st.error(f"‚ùå Camera error: {str(e)}")
                st.session_state.camera_active = False
        
        else:
            st.info("üì∑ **Click 'Start Camera' to begin real-time activity detection**")
            
            st.markdown("""
            ### üéØ How it works:
            1. **Click 'Start Camera'** to activate your webcam
            2. **Position yourself** in front of the camera with good lighting
            3. **Perform activities** - the AI will detect and classify them in real-time
            4. **View confidence scores** to see how certain the model is about each prediction
            5. **Click 'Stop Camera'** when done
            
            ### üí° Tips for best results:
            - Ensure good lighting and minimal background clutter
            - Stand at a comfortable distance from the camera (2-3 meters)
            - Perform clear, distinct movements
            - Allow camera permissions when prompted by your browser
            """)

    with tab2:
        st.header("Upload an Image")
        uploaded_image = st.file_uploader(
            "Choose an image...", 
            type=['jpg', 'jpeg', 'png', 'bmp'],
            help="Upload an image showing a person performing an activity"
        )
        
        if uploaded_image is not None:
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("Original Image")
                st.image(uploaded_image, caption="Uploaded Image", use_container_width=True)
            
            with col2:
                st.subheader("Processing...")
                with st.spinner("Analyzing pose..."):
                    prediction, prediction_proba, features, image_with_landmarks = process_image(
                        uploaded_image, mp_pose, mp_drawing, pose, model
                    )
                
                if prediction is not None:
                    st.subheader("Results")
                    st.success(f"**Predicted Activity:** {prediction}")
                    
                    # Show confidence
                    st.subheader("Confidence Scores")
                    confidence_df = pd.DataFrame({
                        'Activity': model.classes_,
                        'Confidence': prediction_proba
                    }).sort_values('Confidence', ascending=False)
                    
                    # Create confidence bar chart
                    fig, ax = plt.subplots(figsize=(10, 6))
                    bars = ax.barh(confidence_df['Activity'], confidence_df['Confidence'])
                    ax.set_xlabel('Confidence Score')
                    ax.set_title('Prediction Confidence by Activity')
                    
                    # Color the highest confidence bar differently
                    bars[0].set_color('lightcoral')
                    for bar in bars[1:]:
                        bar.set_color('lightblue')
                    
                    plt.tight_layout()
                    st.pyplot(fig)
                    
                    # Show image with landmarks
                    st.subheader("Pose Landmarks")
                    st.image(image_with_landmarks, caption="Image with Detected Pose", use_container_width=True)
                    
                    # Show extracted features
                    with st.expander("View Extracted Features (Joint Angles)"):
                        features_df = pd.DataFrame([features]).T
                        features_df.columns = ['Angle (degrees)']
                        features_df['Angle (degrees)'] = features_df['Angle (degrees)'].round(2)
                        st.dataframe(features_df)
                else:
                    st.error("No pose detected in the image. Please try another image.")
    
    with tab2:
        st.header("Upload a Video")
        uploaded_video = st.file_uploader(
            "Choose a video...", 
            type=['mp4', 'avi', 'mov'],
            help="Upload a short video (max 100 frames will be processed for demo)"
        )
        
        if uploaded_video is not None:
            st.subheader("Video Preview")
            st.video(uploaded_video)
            
            if st.button("üé¨ Analyze Video"):
                st.subheader("Processing Video...")
                with st.spinner("Analyzing poses in video..."):
                    predictions, processed_frames = process_video(
                        uploaded_video, mp_pose, mp_drawing, pose, model
                    )
                
                if predictions:
                    # Show prediction statistics
                    st.subheader("üìä Analysis Results")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # Prediction distribution
                        prediction_counts = pd.Series(predictions).value_counts()
                        
                        fig, ax = plt.subplots(figsize=(10, 6))
                        bars = ax.bar(prediction_counts.index, prediction_counts.values)
                        ax.set_xlabel('Activity')
                        ax.set_ylabel('Number of Frames')
                        ax.set_title('Activity Distribution in Video')
                        plt.xticks(rotation=45, ha='right')
                        
                        # Add value labels on bars
                        for bar in bars:
                            height = bar.get_height()
                            ax.text(bar.get_x() + bar.get_width()/2., height,
                                   f'{int(height)}', ha='center', va='bottom')
                        
                        plt.tight_layout()
                        st.pyplot(fig)
                    
                    with col2:
                        # Timeline of predictions
                        fig, ax = plt.subplots(figsize=(12, 4))
                        
                        # Create color map for activities
                        unique_activities = list(set(predictions))
                        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_activities)))
                        color_map = dict(zip(unique_activities, colors))
                        
                        y_positions = [unique_activities.index(pred) for pred in predictions]
                        frame_numbers = list(range(len(predictions)))
                        
                        ax.scatter(frame_numbers, y_positions, 
                                 c=[color_map[pred] for pred in predictions], 
                                 s=50, alpha=0.7)
                        
                        ax.set_xlabel('Frame Number')
                        ax.set_ylabel('Activity')
                        ax.set_yticks(range(len(unique_activities)))
                        ax.set_yticklabels(unique_activities)
                        ax.set_title('Activity Timeline')
                        ax.grid(True, alpha=0.3)
                        
                        plt.tight_layout()
                        st.pyplot(fig)
                    
                    # Most common prediction
                    most_common = prediction_counts.index[0]
                    st.success(f"**Most common activity detected:** {most_common} ({prediction_counts.iloc[0]} frames)")
                    
                    # Show some processed frames
                    if processed_frames:
                        st.subheader("Sample Processed Frames")
                        # Show every 10th frame or max 5 frames
                        sample_indices = list(range(0, len(processed_frames), max(1, len(processed_frames)//5)))[:5]
                        
                        cols = st.columns(len(sample_indices))
                        for i, idx in enumerate(sample_indices):
                            with cols[i]:
                                frame_rgb = cv2.cvtColor(processed_frames[idx], cv2.COLOR_BGR2RGB)
                                st.image(frame_rgb, caption=f"Frame {idx}: {predictions[idx]}", use_container_width=True)
                
                else:
                    st.error("No poses detected in the video. Please try another video.")

    with tab4:
        st.header("üìä Model Information")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Model Architecture")
            st.markdown("""
            - **Algorithm:** Random Forest Classifier
            - **Input Features:** 8 joint angles extracted from MediaPipe pose landmarks
            - **Classes:** 5 human activities
            - **Performance:** 98% accuracy on test dataset
            """)
            
            st.subheader("Feature Engineering")
            st.markdown("""
            The model uses the following joint angles as features:
            1. **Left/Right Elbow Angles**
            2. **Left/Right Shoulder Angles**  
            3. **Left/Right Hip Angles**
            4. **Left/Right Knee Angles**
            
            These angles are calculated using the law of cosines between three landmark points.
            """)
        
        with col2:
            st.subheader("Training Dataset")
            st.markdown("""
            - **Source:** Kaggle Human Activity Recognition Video Dataset
            - **Activities:** 5 different human activities
            - **Features:** Pose landmarks extracted using MediaPipe
            - **Preprocessing:** Angle calculation for robustness to camera position
            """)
            
            st.subheader("Model Optimization")
            st.markdown("""
            - **Base Model:** Random Forest with default parameters
            - **Hyperparameter Tuning:** GridSearchCV with cross-validation
            - **Result:** Minimal improvement (98% ‚Üí 98%) showing the base model was already optimal
            """)

if __name__ == "__main__":
    main()